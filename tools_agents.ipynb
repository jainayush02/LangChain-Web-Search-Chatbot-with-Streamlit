{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcaf5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29361126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c627ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=250)\n",
    "arxiv= ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "arxiv.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3109e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=250)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "wiki.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75582cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki, arxiv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471caa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cc571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02276923693716526,\n",
       " 0.010134130716323853,\n",
       " 0.011886735446751118,\n",
       " -0.09669032692909241,\n",
       " -0.0027089761570096016]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_api_key = os.getenv(\"GENAI_API_KEY\")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\", google_api_key=google_api_key)\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "634b0b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A0F2BA37D0>, search_kwargs={})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://huggingface.co/docs/inference-providers/index\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200).split_documents(docs)\n",
    "vector_storedb = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector_storedb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cecfce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hugging_face-search'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(retriever, \"hugging_face-search\",\"Search any information about Why Choose Inference Providers\")\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18093d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " Tool(name='hugging_face-search', description='Search any information about Why Choose Inference Providers', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001A0FCAA5080>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A0F2BA37D0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001A0FCAA51C0>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A0F2BA37D0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [wiki, arxiv, retriever_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "325ae301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(groq_api_key = groq_api_key, model=\"llama-3.1-8b-instant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52a2f6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e74efa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001A0CFC5A980>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001A0CFC5A980>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001A0FD92CE50>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001A0F4574550>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'hugging_face-search', 'description': 'Search any information about Why Choose Inference Providers', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_openai_tools_agent \n",
    "agents = create_openai_tools_agent(llm, tools, prompt)\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "599e152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001A0CFC5A980>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001A0CFC5A980>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001A0FD92CE50>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001A0F4574550>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'hugging_face-search', 'description': 'Search any information about Why Choose Inference Providers', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)), Tool(name='hugging_face-search', description='Search any information about Why Choose Inference Providers', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001A0FCAA5080>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A0F2BA37D0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001A0FCAA51C0>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A0F2BA37D0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##agents Executer\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agents, tools=tools, verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13d84a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `hugging_face-search` with `{'query': 'Inference in hugging face'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mhf auth login # get a read token from hf.co/settings/tokens You can now use the the client with a Python interpreter:   Copied import os\n",
      "from huggingface_hub import InferenceClient\n",
      "\n",
      "Inference Providers   Hugging Faceâ€™s Inference Providers give developers access to hundreds of machine learning models, powered by world-class inference providers. They are also integrated into our client SDKs (for JS and Python), making it easy to explore serverless inference of models on your favorite providers.  Partners Our platform integrates with leading AI infrastructure providers, giving you access to their specialized capabilities through a single, consistent API. Hereâ€™s what each partner supports: Provider Chat completion (LLM) Chat completion (VLM) Feature Extraction Text to Image Text to video Speech to text Cerebras âœ…      Cohere âœ… âœ…     Fal AI    âœ… âœ… âœ… Featherless AI âœ… âœ…     Fireworks âœ… âœ…     Groq âœ… âœ…     HF Inference âœ… âœ… âœ… âœ…  âœ… Hyperbolic âœ… âœ…     Nebius âœ… âœ… âœ… âœ…   Novita âœ… âœ…   âœ…  Nscale âœ… âœ…  âœ…   Public AI âœ…      Replicate    âœ… âœ… âœ… SambaNova âœ…  âœ…    Scaleway âœ…  âœ…    Together âœ… âœ…  âœ…    Why Choose Inference Providers? When you build AI applications, itâ€™s tough to manage\n",
      "\n",
      "Inference Providers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hugging Face\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tModels\n",
      "\n",
      "\t\t\t\t\t\tDatasets\n",
      "\n",
      "\t\t\t\t\t\tSpaces\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tCommunity\n",
      "\t\t\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tDocs\n",
      "\n",
      "\t\t\t\t\t\tEnterprise\n",
      "\n",
      "Pricing\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log In\n",
      "\t\t\t\t\n",
      "Sign Up\n",
      "\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "Inference Providers documentation\n",
      "\t\t\t\n",
      "Inference Providers\n",
      "\n",
      "\n",
      "\n",
      "Inference Providers\n",
      "\n",
      "ðŸ¡ View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersKernelsLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm\n",
      "\n",
      "Search documentation\n",
      "\n",
      "\n",
      "main\n",
      "EN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Get Started\n",
      "\n",
      "\n",
      "Inference Providers\n",
      "Pricing and Billing\n",
      "Hub integration\n",
      "Security\n",
      "\n",
      "\n",
      "Guides\n",
      "\n",
      "Search documentation\n",
      "\n",
      "\n",
      "main\n",
      "EN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Get Started\n",
      "\n",
      "\n",
      "Inference Providers\n",
      "Pricing and Billing\n",
      "Hub integration\n",
      "Security\n",
      "\n",
      "\n",
      "Guides\n",
      "\n",
      "\n",
      "Your First API Call\n",
      "Building Your First AI App\n",
      "Structured Outputs with LLMs\n",
      "Function Calling\n",
      "How to use OpenAI gpt-oss\n",
      "Build an Image Editor\n",
      "VS Code with GitHub Copilot\n",
      "\n",
      "\n",
      "Inference Tasks\n",
      "\n",
      "\n",
      "Chat Completion\n",
      "Feature Extraction\n",
      "Text to Image\n",
      "Text to Video\n",
      "\n",
      "Other Tasks\n",
      "\n",
      "\n",
      "\n",
      "Providers\n",
      "\n",
      "\n",
      "Cerebras\n",
      "Cohere\n",
      "Fal AI\n",
      "Featherless AI\n",
      "Fireworks\n",
      "Groq\n",
      "Hyperbolic\n",
      "HF Inference\n",
      "Nebius\n",
      "Novita\n",
      "Nscale\n",
      "Public AI\n",
      "Replicate\n",
      "SambaNova\n",
      "Scaleway\n",
      "Together\n",
      "\n",
      "Hub API\n",
      "Register as an Inference Provider\n",
      "\n",
      "\n",
      "\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "\t\t\n",
      "\n",
      "Collaborate on models, datasets and Spaces\n",
      "\t\t\t\t\n",
      "\n",
      "Faster examples with accelerated inference\n",
      "\t\t\t\t\n",
      "\n",
      "Switch between documentation themes\n",
      "\t\t\t\t\n",
      "Sign Up\n",
      "to get started\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Hugging face inference providers'}`\n",
      "responded: This information is only available from the hugging face search function, as it is a product specific feature.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: DeepSeek\n",
      "Summary: Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd., doing business as DeepSeek, is a Chinese artificial intelligence (AI) company that develops large language models (LLMs). Based in Hangzhou, Zhejia\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': 'hugging face inference'}`\n",
      "responded: This information is only available from the hugging face search function, as it is a product specific feature.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2024-10-06\n",
      "Title: A Large-Scale Exploit Instrumentation Study of AI/ML Supply Chain Attacks in Hugging Face Models\n",
      "Authors: Beatrice Casey, Joanna C. S. Santos, Mehdi Mirakhorli\n",
      "Summary: The development of machine learning (ML) techniques \u001b[0m\u001b[32;1m\u001b[1;3mNote: The second arxiv result was cut off due to it being too long.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about hugging face inference. Only use the available tools: wikipedia, arxiv, or hugging_face-search. Do not use any other tools.',\n",
       " 'output': 'Note: The second arxiv result was cut off due to it being too long.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "\t\"input\": \"Tell me about hugging face inference. Only use the available tools: wikipedia, arxiv, or hugging_face-search. Do not use any other tools.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f11eb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'machine learning'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Machine learning\n",
      "Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks wit\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': 'machine learning'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2019-09-08\n",
      "Title: Lecture Notes: Optimization for Machine Learning\n",
      "Authors: Elad Hazan\n",
      "Summary: Lecture notes on optimization for machine learning, derived from a course at\n",
      "Princeton University and tutorials given in MLSS, Buenos Aires, as\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `hugging_face-search` with `{'query': 'Why Choose Inference Providers'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mâœ… âœ…   âœ…  Nscale âœ… âœ…  âœ…   Public AI âœ…      Replicate    âœ… âœ… âœ… SambaNova âœ…  âœ…    Scaleway âœ…  âœ…    Together âœ… âœ…  âœ…    Why Choose Inference Providers? When you build AI applications, itâ€™s tough to manage multiple provider APIs, comparing model performance, and dealing with varying reliability. Inference Providers solves these challenges by offering: Instant Access to Cutting-Edge Models: Go beyond mainstream providers to access thousands of specialized models across multiple AI tasks. Whether you need the latest language models, state-of-the-art image generators, or domain-specific embeddings, youâ€™ll find them here. Zero Vendor Lock-in: Unlike being tied to a single providerâ€™s model catalog, you get access to models from Cerebras, Groq, Together AI, Replicate, and more â€” all through one consistent interface. Production-Ready Performance: Built for enterprise workloads with the reliability your applications demand. Hereâ€™s what you can build: Text Generation: Use Large language models with\n",
      "\n",
      "text generation, image generation, document embeddings, NER, summarization, image classification, and more. ðŸ”€ Multi-Provider Support: Easily run models from top-tier providers like fal, Replicate, Sambanova, Together AI, and others. ðŸš€ Scalable & Reliable: Built for high availability and low-latency performance in production environments. ðŸ”§ Developer-Friendly: Simple requests, fast responses, and a consistent developer experience across Python and JavaScript clients. ðŸ‘· Easy to integrate: Drop-in replacement for the OpenAI chat completions API. ðŸ’° Cost-Effective: No extra markup on provider rates.  Getting Started Inference Providers works with your existing development workflow. Whether you prefer Python, JavaScript, or direct HTTP calls, we provide native SDKs and OpenAI-compatible APIs to get you up and running quickly. Weâ€™ll walk through a practical example using deepseek-ai/DeepSeek-V3-0324, a state-of-the-art open-weights conversational model.  Inference Playground Before diving\n",
      "\n",
      "You need fine-grained control over the request/response cycle Youâ€™re working in environments without available client libraries  Next Steps Now that you understand the basics, explore these resources to make the most of Inference Providers: Announcement Blog Post: Learn more about the launch of Inference Providers Pricing and Billing: Understand costs and billing of Inference Providers Hub Integration: Learn how Inference Providers are integrated with the Hugging Face Hub Register as a Provider: Requirements to join our partner network as a provider Hub API: Advanced API features and configuration API Reference: Complete parameter documentation for all supported tasks < > Update on GitHub\n",
      "\n",
      "Inference Providers   Hugging Faceâ€™s Inference Providers give developers access to hundreds of machine learning models, powered by world-class inference providers. They are also integrated into our client SDKs (for JS and Python), making it easy to explore serverless inference of models on your favorite providers.  Partners Our platform integrates with leading AI infrastructure providers, giving you access to their specialized capabilities through a single, consistent API. Hereâ€™s what each partner supports: Provider Chat completion (LLM) Chat completion (VLM) Feature Extraction Text to Image Text to video Speech to text Cerebras âœ…      Cohere âœ… âœ…     Fal AI    âœ… âœ… âœ… Featherless AI âœ… âœ…     Fireworks âœ… âœ…     Groq âœ… âœ…     HF Inference âœ… âœ… âœ… âœ…  âœ… Hyperbolic âœ… âœ…     Nebius âœ… âœ… âœ… âœ…   Novita âœ… âœ…   âœ…  Nscale âœ… âœ…  âœ…   Public AI âœ…      Replicate    âœ… âœ… âœ… SambaNova âœ…  âœ…    Scaleway âœ…  âœ…    Together âœ… âœ…  âœ…    Why Choose Inference Providers? When you build AI applications, itâ€™s tough to manage\u001b[0m\u001b[32;1m\u001b[1;3mThe function call for the summary of the 'arxiv' function call was incomplete. This is the complete function call for the 'arxiv' function:\n",
      "\n",
      " <function=arxiv>{\"query\": \"machine learning\"}</function>\n",
      "\n",
      "If you want to know more about the results, you can ask me to clarify any part of the response.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is machine learning',\n",
       " 'output': 'The function call for the summary of the \\'arxiv\\' function call was incomplete. This is the complete function call for the \\'arxiv\\' function:\\n\\n <function=arxiv>{\"query\": \"machine learning\"}</function>\\n\\nIf you want to know more about the results, you can ask me to clarify any part of the response.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "\t\"input\": \"what is machine learning\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16691365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
